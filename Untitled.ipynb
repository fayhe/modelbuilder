{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hahaha\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "print('hahaha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-04 21:59:11,889 : INFO : collecting all words and their counts\n",
      "2018-12-04 21:59:11,890 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-04 21:59:11,892 : INFO : collected 4 word types from a corpus of 5 raw words and 2 sentences\n",
      "2018-12-04 21:59:11,893 : INFO : Loading a fresh vocabulary\n",
      "2018-12-04 21:59:11,894 : INFO : effective_min_count=1 retains 4 unique words (100% of original 4, drops 0)\n",
      "2018-12-04 21:59:11,895 : INFO : effective_min_count=1 leaves 5 word corpus (100% of original 5, drops 0)\n",
      "2018-12-04 21:59:11,897 : INFO : deleting the raw counts dictionary of 4 items\n",
      "2018-12-04 21:59:11,898 : INFO : sample=0.001 downsamples 4 most-common words\n",
      "2018-12-04 21:59:11,899 : INFO : downsampling leaves estimated 0 word corpus (6.6% of prior 5)\n",
      "2018-12-04 21:59:11,901 : INFO : estimated required memory for 4 words and 100 dimensions: 5200 bytes\n",
      "2018-12-04 21:59:11,902 : INFO : resetting layer weights\n",
      "2018-12-04 21:59:11,905 : INFO : training model with 3 workers on 4 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-12-04 21:59:11,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 21:59:11,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 21:59:11,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 21:59:11,914 : INFO : EPOCH - 1 : training on 5 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-12-04 21:59:11,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 21:59:11,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 21:59:11,919 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 21:59:11,920 : INFO : EPOCH - 2 : training on 5 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-12-04 21:59:11,924 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 21:59:11,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 21:59:11,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 21:59:11,927 : INFO : EPOCH - 3 : training on 5 raw words (1 effective words) took 0.0s, 257 effective words/s\n",
      "2018-12-04 21:59:11,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 21:59:11,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 21:59:11,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 21:59:11,934 : INFO : EPOCH - 4 : training on 5 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-12-04 21:59:11,938 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 21:59:11,939 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 21:59:11,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 21:59:11,941 : INFO : EPOCH - 5 : training on 5 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-12-04 21:59:11,943 : INFO : training on a 25 raw words (1 effective words) took 0.0s, 28 effective words/s\n",
      "2018-12-04 21:59:11,944 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', ' ', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.6257188e-04,  3.9804382e-03, -8.6044334e-04, -2.7949782e-03,\n",
       "        3.6360434e-05, -2.1911759e-03, -2.6670028e-03,  3.4931516e-03,\n",
       "       -9.4468531e-05,  3.3975861e-03, -1.9528179e-03,  1.4868926e-03,\n",
       "       -2.4996675e-03,  2.5263156e-03, -4.9121742e-04, -2.0585442e-03,\n",
       "       -2.7208976e-03, -4.7226106e-03,  1.9127486e-03, -5.6878769e-05,\n",
       "        2.9837601e-03,  4.0102276e-04, -4.6822233e-03, -3.8902650e-03,\n",
       "        4.0783831e-03,  3.3700485e-03, -8.4811763e-04, -7.6878793e-04,\n",
       "        1.1810036e-03,  1.0843597e-03,  8.1985467e-04,  3.3292777e-03,\n",
       "        2.0096018e-03, -3.0892121e-03, -1.4212386e-03, -4.9905269e-03,\n",
       "        4.2774398e-03,  1.2798859e-03, -1.0678397e-03, -9.1561960e-04,\n",
       "        4.8698022e-04,  3.8495942e-04, -3.1343026e-03,  3.1514007e-03,\n",
       "        2.4507826e-03,  4.9032550e-03, -2.7298010e-03, -4.4016095e-04,\n",
       "       -1.3101703e-03,  9.0540515e-04,  2.4259076e-03,  4.7122510e-03,\n",
       "       -2.4407592e-03,  4.1281255e-03,  2.8635086e-03,  1.2748400e-03,\n",
       "        3.2925899e-03,  2.4460086e-03,  2.0586811e-03, -2.8069890e-03,\n",
       "        1.5119791e-03,  1.1354989e-03, -3.8077782e-03,  7.2881422e-04,\n",
       "       -5.2906061e-04,  4.1962941e-03, -9.5725281e-04,  2.2085763e-03,\n",
       "       -3.9436249e-03, -1.4655882e-03,  9.3750708e-04,  2.2592368e-03,\n",
       "       -4.3925564e-03, -3.2624535e-03, -2.1571461e-03,  2.8631252e-03,\n",
       "        3.0046736e-03, -1.7885112e-03,  4.8972247e-04,  7.6878336e-05,\n",
       "        2.7564778e-03, -9.7916613e-04,  3.9317997e-04, -1.7027813e-03,\n",
       "       -3.5852182e-03,  3.4571444e-03,  3.5505497e-03,  3.9824480e-03,\n",
       "        2.3560561e-03,  3.0530086e-03, -1.5698627e-03,  1.9454886e-03,\n",
       "       -4.1662459e-03,  4.5698662e-03,  2.8636169e-03,  1.4549643e-03,\n",
       "        2.3034168e-05, -3.1933803e-03,  8.4459310e-04,  1.0354463e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[' '] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
