{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_raw = json.load(open('../dict/vocab_list.json', 'r'))\n",
    "\n",
    "#clean _num_ \n",
    "num_keys = [i for i in vocab_raw.keys() if (i.find('_num_') != -1)]\n",
    "for i in num_keys:\n",
    "    vocab_raw.pop(i)\n",
    "\n",
    "#new key list\n",
    "vocab_keys = [u'_num_']\n",
    "vocab_keys.extend(vocab_raw.keys())\n",
    "\n",
    "#reserve index 0: padding, index 1: _num_\n",
    "vocab_dict = {k:v for k, v in zip(vocab_keys, range(2, len(vocab_keys)+2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'limited': 3,\n",
       " u'hitch': 4,\n",
       " u'four': 5,\n",
       " u'asian': 6,\n",
       " u'captain': 7,\n",
       " u'integrity': 8,\n",
       " u'btec': 9,\n",
       " u'consider': 1318,\n",
       " u'whose': 11,\n",
       " u'calculate': 12,\n",
       " u'aug': 13,\n",
       " u'electricity': 14,\n",
       " u'tweet': 15,\n",
       " u'disability': 81,\n",
       " u'under': 17,\n",
       " u'sorry': 18,\n",
       " u'worth': 19,\n",
       " u'collaborate': 20,\n",
       " u'risk': 21,\n",
       " u'dollar': 1322,\n",
       " u'regional': 23,\n",
       " u'every': 24,\n",
       " u'affect': 25,\n",
       " u'encounter': 26,\n",
       " u'amran': 27,\n",
       " u'school': 28,\n",
       " u'progression': 29,\n",
       " u'good': 1890,\n",
       " u'cmc': 30,\n",
       " u'wednesday': 31,\n",
       " u'convenience': 32,\n",
       " u'enhance': 33,\n",
       " u'ups': 34,\n",
       " u'confidence': 2042,\n",
       " u'enjoy': 35,\n",
       " u'chew': 36,\n",
       " u'force': 37,\n",
       " u'japanese': 38,\n",
       " u'nair': 39,\n",
       " u'second': 40,\n",
       " u'chee': 41,\n",
       " u'chen': 42,\n",
       " u'even': 43,\n",
       " u'neo': 44,\n",
       " u'nel': 45,\n",
       " u'worsen': 46,\n",
       " u'profitability': 47,\n",
       " u'asia': 48,\n",
       " u'spokesman': 49,\n",
       " u'haze': 50,\n",
       " u'above': 51,\n",
       " u'conduct': 52,\n",
       " u'supplier': 53,\n",
       " u'new': 54,\n",
       " u'net': 55,\n",
       " u'officially': 56,\n",
       " u'specialist': 57,\n",
       " u'reporter': 58,\n",
       " u'never': 59,\n",
       " u'here': 60,\n",
       " u'protection': 61,\n",
       " u'china': 62,\n",
       " u'active': 63,\n",
       " u'aftermath': 64,\n",
       " u'celebration': 66,\n",
       " u'property': 67,\n",
       " u'daughter': 68,\n",
       " u'forum': 69,\n",
       " u'study': 70,\n",
       " u'credit': 71,\n",
       " u'smoke': 72,\n",
       " u'military': 73,\n",
       " u'pack': 1775,\n",
       " u'punishment': 75,\n",
       " u'criticism': 76,\n",
       " u'golden': 77,\n",
       " u'secure': 78,\n",
       " u'geylang': 79,\n",
       " u'mobility': 80,\n",
       " u'scold': 16,\n",
       " u'replace': 82,\n",
       " u'patience': 83,\n",
       " u'total': 84,\n",
       " u'unit': 85,\n",
       " u'would': 566,\n",
       " u'hospital': 86,\n",
       " u'achieve': 2300,\n",
       " u'negative': 87,\n",
       " u'foster': 1332,\n",
       " u'call': 89,\n",
       " u'therefore': 90,\n",
       " u'recommend': 92,\n",
       " u'strike': 93,\n",
       " u'survive': 94,\n",
       " u'type': 96,\n",
       " u'until': 97,\n",
       " u'relay': 98,\n",
       " u'successful': 99,\n",
       " u'wars': 100,\n",
       " u'loong': 101,\n",
       " u'hurt': 102,\n",
       " u'warn': 103,\n",
       " u'glass': 104,\n",
       " u'excellent': 105,\n",
       " u'hold': 106,\n",
       " u'must': 107,\n",
       " u'me': 108,\n",
       " u'md': 109,\n",
       " u'intake': 110,\n",
       " u'join': 111,\n",
       " u'room': 684,\n",
       " u'queenstown': 113,\n",
       " u'mo': 114,\n",
       " u'work': 115,\n",
       " u'zaobao': 2308,\n",
       " u'mp': 116,\n",
       " u'install': 725,\n",
       " u'mr': 118,\n",
       " u'root': 119,\n",
       " u'example': 120,\n",
       " u'impose': 121,\n",
       " u'estate': 122,\n",
       " u'give': 123,\n",
       " u'involve': 124,\n",
       " u'authorities': 125,\n",
       " u'want': 126,\n",
       " u'kpis': 127,\n",
       " u'times': 2186,\n",
       " u'attract': 129,\n",
       " u'ceremony': 130,\n",
       " u'end': 131,\n",
       " u'recovery': 132,\n",
       " u'eng': 133,\n",
       " u'provide': 134,\n",
       " u'travel': 135,\n",
       " u'feature': 136,\n",
       " u'machine': 137,\n",
       " u'how': 138,\n",
       " u'hot': 139,\n",
       " u'hop': 140,\n",
       " u'interview': 141,\n",
       " u'gate': 142,\n",
       " u'faulty': 143,\n",
       " u'lan': 144,\n",
       " u'rise': 145,\n",
       " u'longspan': 146,\n",
       " u'earlier': 147,\n",
       " u'lab': 148,\n",
       " u'lay': 149,\n",
       " u'bedok': 150,\n",
       " u'lau': 151,\n",
       " u'law': 152,\n",
       " u'parallel': 153,\n",
       " u'purchase': 154,\n",
       " u'attempt': 155,\n",
       " u'third': 156,\n",
       " u'amid': 157,\n",
       " u'grant': 1782,\n",
       " u'greet': 159,\n",
       " u'think': 160,\n",
       " u's.league': 161,\n",
       " u'maintain': 162,\n",
       " u'green': 163,\n",
       " u'ambulance': 164,\n",
       " u'preventive': 165,\n",
       " u'operate': 166,\n",
       " u'order': 167,\n",
       " u'operations': 168,\n",
       " u'modesty': 169,\n",
       " u'kuek': 170,\n",
       " u'feedback': 171,\n",
       " u'office': 172,\n",
       " u'deck': 173,\n",
       " u'over': 174,\n",
       " u'fall': 1342,\n",
       " u'london': 176,\n",
       " u'innovative': 177,\n",
       " u'japan': 178,\n",
       " u'exchange': 2007,\n",
       " u'before': 179,\n",
       " u'fit': 180,\n",
       " u'personal': 181,\n",
       " u'fix': 182,\n",
       " u'revenue': 183,\n",
       " u',': 184,\n",
       " u'asyraf': 185,\n",
       " u'crew': 186,\n",
       " u'better': 187,\n",
       " u'uob': 188,\n",
       " u'strand': 1344,\n",
       " u'fin': 190,\n",
       " u'ises': 191,\n",
       " u'eventually': 192,\n",
       " u'them': 193,\n",
       " u'affected': 194,\n",
       " u'weakness': 195,\n",
       " u'safe': 196,\n",
       " u'collide': 197,\n",
       " u'ong': 916,\n",
       " u'break': 199,\n",
       " u'they': 200,\n",
       " u'rental': 201,\n",
       " u'bank': 202,\n",
       " u'netherlands': 203,\n",
       " u'ventilation': 204,\n",
       " u'reasonable': 205,\n",
       " u'each': 206,\n",
       " u'side': 207,\n",
       " u'mean': 208,\n",
       " u'consolidate': 209,\n",
       " u'signing': 210,\n",
       " u'financial': 212,\n",
       " u'independent': 213,\n",
       " u'venue': 214,\n",
       " u'solution': 215,\n",
       " u'contracting': 216,\n",
       " u'trading': 217,\n",
       " u'laboratory': 218,\n",
       " u'jiakeng': 219,\n",
       " u'saturday': 220,\n",
       " u'network': 221,\n",
       " u'raffles': 222,\n",
       " u'smu': 223,\n",
       " u'tomorrow': 224,\n",
       " u'content': 225,\n",
       " u're': 226,\n",
       " u'encourage': 227,\n",
       " u'safety': 1545,\n",
       " u'reader': 228,\n",
       " u'surprise': 229,\n",
       " u'newly': 230,\n",
       " u'engineer': 231,\n",
       " u'foundation': 233,\n",
       " u'perception': 234,\n",
       " u'barrier': 235,\n",
       " u'resume': 236,\n",
       " u'ntu': 237,\n",
       " u'free': 238,\n",
       " u'standard': 239,\n",
       " u'tian': 240,\n",
       " u'struggle': 241,\n",
       " u'nta': 242,\n",
       " u'estimate': 243,\n",
       " u'khatib': 244,\n",
       " u'renew': 245,\n",
       " u'blackout': 1355,\n",
       " u'onto': 247,\n",
       " u'user': 1798,\n",
       " u'already': 250,\n",
       " u'ownership': 251,\n",
       " u'render': 252,\n",
       " u'primary': 253,\n",
       " u'rank': 254,\n",
       " u'instruct': 255,\n",
       " u'another': 257,\n",
       " u'insulation': 258,\n",
       " u'bicycle': 259,\n",
       " u'traction': 930,\n",
       " u'service': 261,\n",
       " u'top': 262,\n",
       " u'teo': 1358,\n",
       " u'approximately': 264,\n",
       " u'toh': 265,\n",
       " u'master': 266,\n",
       " u'too': 267,\n",
       " u'toa': 268,\n",
       " u'inflow': 269,\n",
       " u'asset': 270,\n",
       " u'mechanism': 2252,\n",
       " u'serve': 271,\n",
       " u'direct': 272,\n",
       " u'incur': 273,\n",
       " u'western': 274,\n",
       " u'devan': 275,\n",
       " u'distance': 277,\n",
       " u'minibus': 278,\n",
       " u'target': 279,\n",
       " u'ew': 2586,\n",
       " u'hike': 280,\n",
       " u'likely': 281,\n",
       " u'project': 282,\n",
       " u'matter': 283,\n",
       " u'street': 284,\n",
       " u'bidder': 285,\n",
       " u'acquisition': 286,\n",
       " u'enhancement': 1363,\n",
       " u'bridge': 288,\n",
       " u'fashion': 289,\n",
       " u'ming': 290,\n",
       " u'seem': 291,\n",
       " u'regulate': 292,\n",
       " u'seek': 293,\n",
       " u'strength': 295,\n",
       " u'infocomm': 296,\n",
       " u'escalator': 297,\n",
       " u'concrete': 1806,\n",
       " u'latter': 299,\n",
       " u'responsible': 300,\n",
       " u'-': 301,\n",
       " u'isolated': 302,\n",
       " u'thorough': 303,\n",
       " u'contact': 304,\n",
       " u'forces': 305,\n",
       " u'sleeper': 306,\n",
       " u'rider': 307,\n",
       " u'though': 308,\n",
       " u'timah': 309,\n",
       " u'object': 310,\n",
       " u'what': 1369,\n",
       " u'regular': 312,\n",
       " u'letter': 313,\n",
       " u'khoo': 314,\n",
       " u'phase': 315,\n",
       " u'panic': 2149,\n",
       " u'episode': 316,\n",
       " u'professor': 317,\n",
       " u'accreditation': 1372,\n",
       " u'm': 319,\n",
       " u'dog': 320,\n",
       " u'reputation': 321,\n",
       " u'tech': 322,\n",
       " u'teck': 323,\n",
       " u'consumer': 324,\n",
       " u'section': 1374,\n",
       " u'advisory': 326,\n",
       " u'inspire': 327,\n",
       " u'alight': 328,\n",
       " u'probe': 329,\n",
       " u'chartered': 330,\n",
       " u'serangoon': 331,\n",
       " u'availability': 332,\n",
       " u'heong': 333,\n",
       " u'busy': 334,\n",
       " u'just': 335,\n",
       " u'implementation': 336,\n",
       " u'explain': 337,\n",
       " u'siew': 338,\n",
       " u'cripple': 339,\n",
       " u'commence': 2225,\n",
       " u'theme': 341,\n",
       " u'screening': 342,\n",
       " u'integrate': 343,\n",
       " u'announce': 344,\n",
       " u'adequate': 345,\n",
       " u'meng': 346,\n",
       " u'oct': 347,\n",
       " u'do': 348,\n",
       " u'newton': 349,\n",
       " u'stop': 350,\n",
       " u'coast': 351,\n",
       " u'christopher': 352,\n",
       " u'tammy': 353,\n",
       " u'despite': 354,\n",
       " u'report': 355,\n",
       " u'dr': 356,\n",
       " u'earn': 357,\n",
       " u'spokesperson': 358,\n",
       " u'ever': 359,\n",
       " u'bay': 361,\n",
       " u'twice': 362,\n",
       " u'bad': 363,\n",
       " u'release': 364,\n",
       " u'steal': 365,\n",
       " u'secretary': 366,\n",
       " u'respond': 367,\n",
       " u'leasing': 368,\n",
       " u'human': 369,\n",
       " u'observer': 370,\n",
       " u'nus': 371,\n",
       " u'detection': 372,\n",
       " u'result': 373,\n",
       " u'fail': 374,\n",
       " u'alternative': 276,\n",
       " u'best': 376,\n",
       " u'capacity': 377,\n",
       " u'away': 378,\n",
       " u'unable': 379,\n",
       " u'ocbc': 380,\n",
       " u'approach': 381,\n",
       " u'wage': 1821,\n",
       " u'accord': 383,\n",
       " u'men': 384,\n",
       " u'omg': 385,\n",
       " u'extend': 386,\n",
       " u'nature': 387,\n",
       " u'confusion': 388,\n",
       " u'weak': 389,\n",
       " u'however': 390,\n",
       " u'vernon': 391,\n",
       " u'wear': 392,\n",
       " u'huang': 393,\n",
       " u'wp': 394,\n",
       " u'news': 395,\n",
       " u'improve': 396,\n",
       " u'protect': 397,\n",
       " u'accident': 398,\n",
       " u'candidate': 399,\n",
       " u'country': 400,\n",
       " u'uncertainty': 401,\n",
       " u'against': 402,\n",
       " u'coe': 403,\n",
       " u'beeline': 404,\n",
       " u'contribution': 405,\n",
       " u'argue': 406,\n",
       " u'ease': 407,\n",
       " u'negotiation': 408,\n",
       " u'pre': 409,\n",
       " u'tough': 411,\n",
       " u'tong': 412,\n",
       " u'initiative': 413,\n",
       " u'trust': 414,\n",
       " u'speak': 416,\n",
       " u'conference': 417,\n",
       " u'recur': 418,\n",
       " u'basis': 419,\n",
       " u'union': 420,\n",
       " u'three': 421,\n",
       " u'.': 422,\n",
       " u'ting': 423,\n",
       " u'confident': 424,\n",
       " u'much': 425,\n",
       " u'stadium': 426,\n",
       " u'basic': 427,\n",
       " u'ihpc': 428,\n",
       " u'temasek': 429,\n",
       " u'location': 1830,\n",
       " u'website': 431,\n",
       " u'life': 432,\n",
       " u'chua': 433,\n",
       " u'chun': 434,\n",
       " u'worker': 435,\n",
       " u'lift': 436,\n",
       " u'child': 437,\n",
       " u'catch': 438,\n",
       " u'save': 439,\n",
       " u'publicly': 440,\n",
       " u'tang': 441,\n",
       " u'heartland': 442,\n",
       " u'air': 443,\n",
       " u'aim': 444,\n",
       " u'near': 445,\n",
       " u'suppose': 446,\n",
       " u'motorist': 447,\n",
       " u'aid': 448,\n",
       " u'voice': 449,\n",
       " u'procedure': 450,\n",
       " u'seven': 451,\n",
       " u'sticker': 452,\n",
       " u'it': 453,\n",
       " u'player': 454,\n",
       " u'straits': 455,\n",
       " u'brake': 456,\n",
       " u'in': 457,\n",
       " u'if': 1224,\n",
       " u'hong': 459,\n",
       " u'perform': 460,\n",
       " u'hsien': 461,\n",
       " u'make': 462,\n",
       " u'pasir': 463,\n",
       " u'amount': 1392,\n",
       " u'beside': 465,\n",
       " u'grapple': 466,\n",
       " u'complex': 467,\n",
       " u'potentially': 468,\n",
       " u'evaluate': 1837,\n",
       " u'several': 470,\n",
       " u'couple': 1838,\n",
       " u'wheel': 472,\n",
       " u'fairly': 473,\n",
       " u'emphasise': 474,\n",
       " u'pick': 1394,\n",
       " u'rail': 476,\n",
       " u'workforce': 477,\n",
       " u'rain': 478,\n",
       " u'hand': 479,\n",
       " u'kio': 480,\n",
       " u'kin': 481,\n",
       " u'nationalisation': 482,\n",
       " u'opportunity': 484,\n",
       " u'scenario': 485,\n",
       " u'academic': 486,\n",
       " u'client': 487,\n",
       " u'mother': 488,\n",
       " u'sengkang': 489,\n",
       " u'the': 490,\n",
       " u'corporate': 491,\n",
       " u'joon': 492,\n",
       " u'left': 493,\n",
       " u'protocol': 494,\n",
       " u'yeo': 495,\n",
       " u'sentence': 496,\n",
       " u'photo': 497,\n",
       " u'laptop': 498,\n",
       " u'distribute': 499,\n",
       " u'yee': 500,\n",
       " u'identify': 501,\n",
       " u'thanks': 502,\n",
       " u'victim': 503,\n",
       " u'yet': 504,\n",
       " u'yew': 505,\n",
       " u'previous': 506,\n",
       " u'campaign': 507,\n",
       " u'han': 508,\n",
       " u'citycab': 509,\n",
       " u'character': 510,\n",
       " u'ideal': 972,\n",
       " u'700a': 1401,\n",
       " u'spread': 513,\n",
       " u'board': 514,\n",
       " u'easy': 515,\n",
       " u'technological': 516,\n",
       " u'consideration': 2016,\n",
       " u'east': 517,\n",
       " u'city': 518,\n",
       " u'designate': 519,\n",
       " u'possible': 521,\n",
       " u'possibly': 522,\n",
       " u'judge': 523,\n",
       " u'highly': 524,\n",
       " u'advanced': 525,\n",
       " u'apart': 526,\n",
       " u'comfortdelgro': 528,\n",
       " u'specific': 529,\n",
       " u'remind': 530,\n",
       " u'officer': 531,\n",
       " u'night': 532,\n",
       " u'security': 533,\n",
       " u'oncoming': 1845,\n",
       " u'right': 535,\n",
       " u'old': 536,\n",
       " u'crowd': 537,\n",
       " u'people': 538,\n",
       " u'dead': 540,\n",
       " u'malaysia': 541,\n",
       " u'mileage': 2259,\n",
       " u'election': 542,\n",
       " u'statistic': 543,\n",
       " u'for': 544,\n",
       " u'bottom': 545,\n",
       " u'lhzb': 1406,\n",
       " u'normal': 547,\n",
       " u'aedge': 548,\n",
       " u'arrest': 2452,\n",
       " u'cna': 550,\n",
       " u'expense': 2423,\n",
       " u'foo': 551,\n",
       " u'christmas': 552,\n",
       " u'booking': 553,\n",
       " u'core': 554,\n",
       " u'particular': 1429,\n",
       " u'marketing': 556,\n",
       " u'burn': 723,\n",
       " u'ply': 559,\n",
       " u'discount': 560,\n",
       " u'corp': 746,\n",
       " u'peer': 753,\n",
       " u'post': 563,\n",
       " u'qingdao': 564,\n",
       " u'chuan': 565,\n",
       " u'plus': 802,\n",
       " u'afternoon': 568,\n",
       " u'commit': 569,\n",
       " u'scdf': 818,\n",
       " u'respondent': 571,\n",
       " u'presence': 846,\n",
       " u'civil': 573,\n",
       " u'son': 574,\n",
       " u'down': 575,\n",
       " u'device': 2269,\n",
       " u'paya': 577,\n",
       " u'parade': 1852,\n",
       " u'wan': 579,\n",
       " u'rely': 580,\n",
       " u'deal': 581,\n",
       " u'wah': 582,\n",
       " u'support': 583,\n",
       " u'initial': 584,\n",
       " u'wab': 585,\n",
       " u'fight': 586,\n",
       " u'editor': 587,\n",
       " u'way': 588,\n",
       " u'overhead': 589,\n",
       " u'war': 590,\n",
       " u'happy': 591,\n",
       " u'head': 592,\n",
       " u'medium': 593,\n",
       " u'form': 594,\n",
       " u'offer': 595,\n",
       " u'analyse': 597,\n",
       " u'failure': 598,\n",
       " u'hear': 599,\n",
       " u'catalyst': 600,\n",
       " u'solar': 601,\n",
       " u'safeguard': 602,\n",
       " u'analyst': 603,\n",
       " u'mtrc': 604,\n",
       " u'mtr': 605,\n",
       " u'limousine': 606,\n",
       " u'inside': 607,\n",
       " u'netizens': 609,\n",
       " u'maximum': 610,\n",
       " u'lorry': 849,\n",
       " u'decorate': 612,\n",
       " u'passenger': 613,\n",
       " u'adopt': 993,\n",
       " u'shin': 615,\n",
       " u'mirror': 1858,\n",
       " u'petir': 617,\n",
       " u'singaporeans': 618,\n",
       " u'expertise': 619,\n",
       " u'ship': 1129,\n",
       " u'check': 621,\n",
       " u'physical': 622,\n",
       " u'vista': 623,\n",
       " u'no': 624,\n",
       " u'stake': 625,\n",
       " u'generally': 626,\n",
       " u'ng': 627,\n",
       " u'tim': 628,\n",
       " u'republic': 1180,\n",
       " u'holding': 630,\n",
       " u'test': 631,\n",
       " u'tie': 632,\n",
       " u'ns': 1198,\n",
       " u'smell': 634,\n",
       " u'roll': 635,\n",
       " u'picture': 636,\n",
       " u\"'s\": 1209,\n",
       " u'why': 2271,\n",
       " u'outage': 1218,\n",
       " u'update': 640,\n",
       " u'journey': 641,\n",
       " u'award': 642,\n",
       " u'variable': 643,\n",
       " u'discharge': 644,\n",
       " u'weekend': 645,\n",
       " u'billion': 646,\n",
       " u'faster': 1266,\n",
       " u'apology': 648,\n",
       " u'disruption': 649,\n",
       " u'assume': 650,\n",
       " u'interval': 651,\n",
       " u'daily': 1285,\n",
       " u'time': 653,\n",
       " u'push': 654,\n",
       " u'serious': 655,\n",
       " u'concept': 658,\n",
       " u'themed': 659,\n",
       " u'skip': 660,\n",
       " u'global': 661,\n",
       " u'focus': 662,\n",
       " u'manager': 663,\n",
       " u'grateful': 664,\n",
       " u'tampines': 665,\n",
       " u'certainly': 666,\n",
       " u'jurong': 667,\n",
       " u'father': 669,\n",
       " u'environment': 670,\n",
       " u'charge': 671,\n",
       " u'compensate': 672,\n",
       " u'oversight': 673,\n",
       " u'division': 674,\n",
       " u'spate': 675,\n",
       " u'choice': 676,\n",
       " u'embark': 677,\n",
       " u'lrt': 678,\n",
       " u'word': 679,\n",
       " u'corp.': 680,\n",
       " u'exact': 681,\n",
       " u'dhoby': 682,\n",
       " u'minute': 683,\n",
       " u'tnp': 112,\n",
       " u'buhari': 685,\n",
       " u'level': 686,\n",
       " u'tear': 687,\n",
       " u'die': 688,\n",
       " u'bukit': 689,\n",
       " u'accidentally': 690,\n",
       " u'leave': 691,\n",
       " u'item': 692,\n",
       " u'settle': 693,\n",
       " u'excellence': 694,\n",
       " u'team': 695,\n",
       " u'quick': 696,\n",
       " u'speculation': 697,\n",
       " u'dip': 698,\n",
       " u'round': 699,\n",
       " u'unaware': 700,\n",
       " u'prevent': 701,\n",
       " u'occurrence': 702,\n",
       " u'trend': 703,\n",
       " u'discover': 704,\n",
       " u'cyber': 705,\n",
       " u'sign': 706,\n",
       " u'colin': 707,\n",
       " u'cost': 708,\n",
       " u'voucher': 709,\n",
       " u'run': 1867,\n",
       " u'dbs': 2283,\n",
       " u'appear': 712,\n",
       " u'assistance': 713,\n",
       " u'mall': 1439,\n",
       " u'current': 715,\n",
       " u'suspect': 717,\n",
       " u'international': 719,\n",
       " u'appeal': 720,\n",
       " u'boost': 721,\n",
       " u'hadi': 2287,\n",
       " u'pose': 558,\n",
       " u'transportation': 724,\n",
       " u'transparency': 117,\n",
       " u'funeral': 726,\n",
       " u'understanding': 727,\n",
       " u'water': 728,\n",
       " u'changi': 729,\n",
       " u'upgrade': 730,\n",
       " u'address': 731,\n",
       " u'along': 732,\n",
       " u'change': 733,\n",
       " u'wait': 734,\n",
       " u'box': 735,\n",
       " u'boy': 736,\n",
       " u'boast': 737,\n",
       " u'institute': 738,\n",
       " u'shift': 739,\n",
       " u'guilty': 740,\n",
       " u'buslines': 741,\n",
       " u'queue': 742,\n",
       " u'trial': 743,\n",
       " u'suggestion': 744,\n",
       " u'usually': 745,\n",
       " u'newcomer': 561,\n",
       " u'taipei': 747,\n",
       " u'emerge': 748,\n",
       " u'love': 749,\n",
       " u'extra': 750,\n",
       " u'merely': 751,\n",
       " u'prefer': 752,\n",
       " u'cabby': 562,\n",
       " u'licensing': 754,\n",
       " u'instal': 755,\n",
       " u'employer': 2229,\n",
       " u'crisis': 757,\n",
       " u'market': 758,\n",
       " u'terminal': 2295,\n",
       " u'prove': 760,\n",
       " u'positive': 761,\n",
       " u'swee': 762,\n",
       " u'france': 763,\n",
       " u'live': 764,\n",
       " u'australian': 766,\n",
       " u'today': 767,\n",
       " u'entrance': 768,\n",
       " u'sharing': 769,\n",
       " u'club': 770,\n",
       " u'maximise': 771,\n",
       " u'downtown': 772,\n",
       " u'fuel': 2297,\n",
       " u'effort': 774,\n",
       " u'behalf': 775,\n",
       " u'impair': 776,\n",
       " u'car': 777,\n",
       " u'cap': 778,\n",
       " u'ghaut': 779,\n",
       " u'can': 780,\n",
       " u'cab': 781,\n",
       " u'arrive': 782,\n",
       " u'heart': 783,\n",
       " u'dedicated': 784,\n",
       " u'figure': 785,\n",
       " u'december': 786,\n",
       " u'sense': 1452,\n",
       " u'topic': 788,\n",
       " u'chin': 789,\n",
       " u'council': 790,\n",
       " u'chia': 791,\n",
       " u'occur': 792,\n",
       " u'spf': 793,\n",
       " u'byung': 794,\n",
       " u'redundancy': 795,\n",
       " u'monitoring': 796,\n",
       " u'discussion': 797,\n",
       " u'write': 798,\n",
       " u'till': 799,\n",
       " u'sunday': 800,\n",
       " u'fourth': 801,\n",
       " u'ensure': 567,\n",
       " u'economy': 803,\n",
       " u'product': 804,\n",
       " u'mar': 805,\n",
       " u'information': 1455,\n",
       " u'may': 807,\n",
       " u'spot': 809,\n",
       " u'lucky': 2305,\n",
       " u'sustainability': 810,\n",
       " u'date': 811,\n",
       " u'such': 812,\n",
       " u'data': 813,\n",
       " u'grow': 814,\n",
       " u'man': 815,\n",
       " u'stress': 816,\n",
       " u'conscious': 817,\n",
       " u'slightly': 570,\n",
       " u'sq': 819,\n",
       " u'applicant': 820,\n",
       " u'benny': 821,\n",
       " u'st': 822,\n",
       " u'inform': 823,\n",
       " u'switch': 824,\n",
       " u'so': 825,\n",
       " u'progressively': 826,\n",
       " u'sm': 827,\n",
       " u'gerard': 828,\n",
       " u'sg': 829,\n",
       " u'talk': 830,\n",
       " u'outlet': 1024,\n",
       " u'refund': 832,\n",
       " u'josephine': 833,\n",
       " u'mainly': 834,\n",
       " u'unacceptable': 835,\n",
       " u'kong': 836,\n",
       " u'course': 837,\n",
       " u'equip': 838,\n",
       " u'still': 839,\n",
       " u'foldable': 840,\n",
       " u'derive': 841,\n",
       " u'overhaul': 842,\n",
       " u'monitor': 843,\n",
       " u'acknowledge': 844,\n",
       " u'organisation': 845,\n",
       " u'facility': 572,\n",
       " u'platform': 847,\n",
       " u'window': 848,\n",
       " u'policy': 850,\n",
       " u'main': 851,\n",
       " u'prime': 852,\n",
       " u'non': 853,\n",
       " u'halt': 855,\n",
       " u'shatter': 856,\n",
       " u'introduce': 857,\n",
       " u'nation': 858,\n",
       " u'answer': 859,\n",
       " u'half': 860,\n",
       " u'not': 861,\n",
       " u'nov': 862,\n",
       " u'now': 863,\n",
       " u'hall': 864,\n",
       " u'nor': 865,\n",
       " u'term': 866,\n",
       " u'name': 867,\n",
       " u'january': 868,\n",
       " u'drop': 869,\n",
       " u'wong': 870,\n",
       " u'realise': 871,\n",
       " u'quarter': 872,\n",
       " u'bridging': 873,\n",
       " u'retrieve': 874,\n",
       " u'significantly': 875,\n",
       " u'kang': 876,\n",
       " u'sponsor': 877,\n",
       " u'ez': 878,\n",
       " u'girl': 879,\n",
       " u'summary': 1033,\n",
       " u'living': 881,\n",
       " u'space': 882,\n",
       " u'profit': 883,\n",
       " u'furthermore': 884,\n",
       " u'increase': 885,\n",
       " u'seriously': 886,\n",
       " u'investigation': 887,\n",
       " u'internet': 888,\n",
       " u'formula': 889,\n",
       " u'sensor': 890,\n",
       " u'she': 2311,\n",
       " u'after': 892,\n",
       " u'million': 893,\n",
       " u'possibility': 894,\n",
       " u'quite': 895,\n",
       " u'besides': 896,\n",
       " u'marina': 897,\n",
       " u'marine': 898,\n",
       " u'card': 899,\n",
       " u'care': 900,\n",
       " u'partner': 2401,\n",
       " u'training': 901,\n",
       " u'ministry': 902,\n",
       " u'transition': 903,\n",
       " u'british': 904,\n",
       " u'initiate': 905,\n",
       " u'place': 906,\n",
       " u'massive': 907,\n",
       " u'promotion': 908,\n",
       " u'neglect': 909,\n",
       " u'frequent': 910,\n",
       " u'first': 911,\n",
       " u'suspend': 912,\n",
       " u'saving': 913,\n",
       " u'remuneration': 914,\n",
       " u'smrts': 915,\n",
       " u'bishan': 198,\n",
       " u'one': 917,\n",
       " u'long': 1898,\n",
       " u'directly': 919,\n",
       " u'president': 920,\n",
       " u'vote': 921,\n",
       " u'message': 922,\n",
       " u'open': 923,\n",
       " u'kuan': 924,\n",
       " u'size': 925,\n",
       " u'interchange': 926,\n",
       " u'little': 927,\n",
       " u'district': 928,\n",
       " u'yio': 929,\n",
       " u'anyone': 260,\n",
       " u'indicate': 931,\n",
       " u'2': 932,\n",
       " u'tragic': 933,\n",
       " u'tell': 611,\n",
       " u'deploy': 2261,\n",
       " u'desmond': 934,\n",
       " u'cite': 935,\n",
       " u'friend': 936,\n",
       " u'allegedly': 937,\n",
       " u'that': 938,\n",
       " u'season': 939,\n",
       " u'hui': 940,\n",
       " u'smrt.': 941,\n",
       " u'850e': 942,\n",
       " u'than': 943,\n",
       " u'population': 944,\n",
       " u'11': 945,\n",
       " u'wide': 946,\n",
       " u':': 1903,\n",
       " u'nathan': 948,\n",
       " u'effective': 949,\n",
       " u'rival': 950,\n",
       " u'require': 951,\n",
       " u'recruit': 952,\n",
       " u'future': 953,\n",
       " u'venture': 954,\n",
       " u'transport': 2495,\n",
       " u'and': 1345,\n",
       " u'hairline': 956,\n",
       " u'ang': 957,\n",
       " u'san': 958,\n",
       " u'premier': 959,\n",
       " u'saf': 960,\n",
       " u'say': 962,\n",
       " u'recover': 963,\n",
       " u'saw': 964,\n",
       " u'any': 965,\n",
       " u'gantry': 966,\n",
       " u'efficient': 967,\n",
       " u'upgrading': 968,\n",
       " u'aside': 969,\n",
       " u'note': 970,\n",
       " u'equipment': 971,\n",
       " u'emphasis': 511,\n",
       " u'potential': 973,\n",
       " u'take': 974,\n",
       " u'online': 975,\n",
       " u'performance': 976,\n",
       " u'wonder': 977,\n",
       " u'channel': 978,\n",
       " u'urge': 979,\n",
       " u'begin': 980,\n",
       " u'sure': 981,\n",
       " u'multiple': 982,\n",
       " u'trace': 983,\n",
       " u'opposite': 984,\n",
       " u'track': 985,\n",
       " u'price': 986,\n",
       " u'enter': 987,\n",
       " u'intergalactic': 988,\n",
       " u'bombardier': 989,\n",
       " u'especially': 990,\n",
       " u'mrt': 991,\n",
       " u'average': 992,\n",
       " u'later': 614,\n",
       " u'drive': 994,\n",
       " u'mrs': 995,\n",
       " u'chauffeured': 996,\n",
       " u'professional': 997,\n",
       " u'senior': 998,\n",
       " u'braddell': 999,\n",
       " u'shop': 1001,\n",
       " u'rating': 1002,\n",
       " u'transdev': 1003,\n",
       " u'show': 1004,\n",
       " u'trespass': 1005,\n",
       " u'berita': 1006,\n",
       " u'shoe': 1007,\n",
       " u'corner': 1008,\n",
       " u'line': 1914,\n",
       " u'pacific': 1910,\n",
       " u'ground': 1010,\n",
       " u'slow': 1011,\n",
       " u'title': 1012,\n",
       " u'behind': 1013,\n",
       " u'only': 1014,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2063)\n"
     ]
    }
   ],
   "source": [
    "def word2Int(wd):\n",
    "    try:\n",
    "        return(vocab_dict[wd])\n",
    "    except KeyError:\n",
    "        return 1\n",
    "\n",
    "print(word2Int(u'nuk'), word2Int(u'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10488, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Cheng's labeled data\n",
    "train_df = pd.read_csv('../NER-label/training-set.csv', quotechar='\\'', names=['POS', 'TEXT', 'ENTITY'])\n",
    "\n",
    "#labeled data\n",
    "#train_df = pd.read_csv('/home/NER-label/NER-marked/marked-training-data.csv', quotechar='\\\"', index_col=False, names=['POS', 'TEXT', 'ENTITY'])\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pos_keys = Counter(train_df['POS']).keys()\n",
    "pos2Int_ = {k:v for k, v in zip(pos_keys, range(len(pos_keys)))}\n",
    "pos2Int = lambda x:pos2Int_[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagging cleaning\n",
    "train_df.ix[train_df['ENTITY'] == 'LOC', 'ENTITY'] = u'O'\n",
    "train_df.ix[train_df['ENTITY'] == ' O', 'ENTITY'] = u'O'\n",
    "train_df.ix[train_df['ENTITY'] == ' ORG', 'ENTITY'] = u'ORG'\n",
    "train_df.ix[train_df['ENTITY'] == '0RG', 'ENTITY'] = u'ORG'\n",
    "train_df.ix[train_df['ENTITY'] == 'DATE/ORG', 'ENTITY'] = u'DATE'\n",
    "train_df.ix[train_df['ENTITY'] == 'FAC', 'ENTITY'] = u'STATION'\n",
    "train_df.ix[train_df['ENTITY'] == 'GPE', 'ENTITY'] = u'COUNTRY'\n",
    "train_df.ix[train_df['ENTITY'] == 'MONEY/ORG', 'ENTITY'] = u'MONEY'\n",
    "train_df.ix[train_df['ENTITY'] == 'OT', 'ENTITY'] = u'LINE'\n",
    "train_df.ix[train_df['ENTITY'] == 'ORDINAL', 'ENTITY'] = u'O'\n",
    "train_df.ix[train_df['ENTITY'] == 'ROAD/STATION', 'ENTITY'] = u'STATION'\n",
    "train_df.ix[train_df['ENTITY'] == 'CARDINAL', 'ENTITY'] = u'GNUM'\n",
    "train_df.ix[train_df['ENTITY'] == 'CARDINAL', 'ENTITY'] = u'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_keys = Counter(train_df['ENTITY']).keys()\n",
    "entity2Int_ = {k:v for k, v in zip(entity_keys, range(len(entity_keys)))}\n",
    "entity2Int = lambda x:entity2Int_[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nan: 0, 'INTERCHANGE': 1, 'SERVICE': 2, 'TITLE': 3, 'DATE': 4, 'STATION': 5, 'DURATION': 6, 'LINE': 7, 'AREA': 8, 'BUS': 9, 'LOCATION': 10, 'ROAD': 11, u'GNUM': 12, 'PERCENT': 13, 'O': 14, 'DEPOT': 15, 'PRODUCT': 16, 'NORP': 17, 'MONEY': 18, 'PERSON': 19, 'TIME': 20, 'ORG': 21, 'EVENT': 22}\n"
     ]
    }
   ],
   "source": [
    "print(entity2Int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nan: 0, 'ADV': 1, 'NOUN': 2, 'ADP': 3, 'PUNCT': 10, 'PROPN': 5, 'DET': 6, 'SYM': 7, 'INTJ': 8, 'PRON': 4, 'PART': 9, 'X': 12, 'NUM': 11, 'CONJ': 13, 'ADJ': 14, 'VERB': 15}\n"
     ]
    }
   ],
   "source": [
    "print(pos2Int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save entity key-value pair\n",
    "json.dump(entity2Int_, open('./Model/entityDict.json', 'w'))\n",
    "json.dump(pos2Int_, open('./Model/posDict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['TEXT'] = train_df['TEXT'].astype('string')\n",
    "\n",
    "sent_break = train_df[train_df['TEXT'] == 'nan'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace the number with '_num_' and words\n",
    "train_df.ix[train_df['POS'] == 'NUM', 'TEXT'] = u'_num_'\n",
    "train_df.ix[train_df['TEXT'] == '\\'ve', 'TEXT'] = u'have'\n",
    "train_df.ix[train_df['TEXT'] == '\\'m', 'TEXT'] = u'am'\n",
    "train_df.ix[train_df['TEXT'] == '\\'re', 'TEXT'] = u'are'\n",
    "train_df.ix[train_df['TEXT'] == '\\'s', 'TEXT'] = u'is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n"
     ]
    }
   ],
   "source": [
    "tmp_v = [-1]\n",
    "tmp_v.extend(sent_break.values)\n",
    "tmp_v.append(train_df.shape[0])\n",
    "\n",
    "POS_DATA = []\n",
    "TEXT_DATA = []\n",
    "ENTITY_DATA = []\n",
    "\n",
    "for i in range(len(tmp_v)-1):\n",
    "    start_i = tmp_v[i] + 1\n",
    "    end_i = tmp_v[i+1]\n",
    "    \n",
    "    if((end_i-start_i)>3): #remove empty string\n",
    "        pos_v = train_df['POS'][start_i:end_i]\n",
    "        if('NUM' in pos_v.values) or ('PROPN' in pos_v.values):\n",
    "            POS_DATA.append(pos_v)\n",
    "            TEXT_DATA.append(train_df['TEXT'][start_i:end_i])\n",
    "            ENTITY_DATA.append(train_df['ENTITY'][start_i:end_i])\n",
    "\n",
    "print(len(ENTITY_DATA))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0            O\n",
      "1            O\n",
      "2            O\n",
      "3          ORG\n",
      "4            O\n",
      "5            O\n",
      "6          BUS\n",
      "7            O\n",
      "8          BUS\n",
      "9            O\n",
      "10           O\n",
      "11           O\n",
      "12           O\n",
      "13        DATE\n",
      "14           O\n",
      "15        DATE\n",
      "16        DATE\n",
      "17           O\n",
      "18           O\n",
      "19           O\n",
      "20    LOCATION\n",
      "21    LOCATION\n",
      "22           O\n",
      "23           O\n",
      "24           O\n",
      "25           O\n",
      "26        ROAD\n",
      "27        ROAD\n",
      "28        ROAD\n",
      "29        ROAD\n",
      "30           O\n",
      "31           O\n",
      "32           O\n",
      "33           O\n",
      "34           O\n",
      "35           O\n",
      "36           O\n",
      "37           O\n",
      "38           O\n",
      "39           O\n",
      "Name: ENTITY, dtype: object, 41       O\n",
      "42       O\n",
      "43       O\n",
      "44       O\n",
      "45       O\n",
      "46       O\n",
      "47       O\n",
      "48       O\n",
      "49       O\n",
      "50       O\n",
      "51    ROAD\n",
      "52    ROAD\n",
      "53       O\n",
      "54       O\n",
      "Name: ENTITY, dtype: object]\n",
      "[0           The\n",
      "1        routes\n",
      "2            of\n",
      "3          SMRT\n",
      "4           bus\n",
      "5      services\n",
      "6         _num_\n",
      "7           and\n",
      "8         _num_\n",
      "9          will\n",
      "10           be\n",
      "11      amended\n",
      "12         from\n",
      "13       Sunday\n",
      "14            (\n",
      "15          May\n",
      "16        _num_\n",
      "17            )\n",
      "18            ,\n",
      "19           as\n",
      "20     Anderson\n",
      "21       Bridge\n",
      "22           in\n",
      "23          the\n",
      "24    direction\n",
      "25           of\n",
      "26           St\n",
      "27       Andrew\n",
      "28           is\n",
      "29         Road\n",
      "30         will\n",
      "31           be\n",
      "32       closed\n",
      "33          off\n",
      "34           to\n",
      "35        buses\n",
      "36          and\n",
      "37        heavy\n",
      "38     vehicles\n",
      "39            0\n",
      "Name: TEXT, dtype: object, 41          The\n",
      "42       routes\n",
      "43           of\n",
      "44         both\n",
      "45     services\n",
      "46         will\n",
      "47           be\n",
      "48      amended\n",
      "49           to\n",
      "50          ply\n",
      "51    Esplanade\n",
      "52        Drive\n",
      "53      instead\n",
      "54            0\n",
      "Name: TEXT, dtype: object]\n"
     ]
    }
   ],
   "source": [
    "from textacy import Doc\n",
    "\n",
    "POS_MAPPING_DATA = [[pos2Int(i) for i in il] for il in POS_DATA]\n",
    "ENTITY_MAPPING_DATA = [[entity2Int(i) for i in il] for il in ENTITY_DATA]\n",
    "\n",
    "def text2lemma(pds):\n",
    "    assert pds.shape[0] > 1\n",
    "    try:\n",
    "        doc_v = Doc(u' '.join(pds))\n",
    "        for i in doc_v:\n",
    "            yield(i.lemma_)\n",
    "    except(RuntimeError):\n",
    "        print(pds)\n",
    "\n",
    "TEXT_MAPPING_DATA = [[word2Int(i) for i in text2lemma(il)] for il in TEXT_DATA]\n",
    "print(ENTITY_DATA[0:2])\n",
    "print(TEXT_DATA[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo sentences: 322\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Number fo sentences: \"+str(len(TEXT_MAPPING_DATA)))\n",
    "print((ENTITY_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Activation, Dense, Dropout, merge, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(embeddings_initializer=\"uniform\", output_dim=200, input_dim=2613)`\n",
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(embeddings_initializer=\"uniform\", output_dim=16, input_dim=16)`\n",
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/fay/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, recurrent_dropout=0.2, dropout=0.2, input_shape=(3, 216), return_sequences=True)`\n",
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ou..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "#multiple input model (word embedding + pos tagging)\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 2+len(vocab_dict.keys())\n",
    "maxlen = 3 # must be odd\n",
    "embedding_dim = 200\n",
    "hidden_dim = 500\n",
    "output_dim = len(entity2Int_.keys())\n",
    "pos_dim = len(pos2Int_.keys())\n",
    "\n",
    "#model input (word embedding)\n",
    "word_input = Input(shape=(maxlen, ), name='word_input')\n",
    "word_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, init='uniform')(word_input)\n",
    "\n",
    "#model input(pos tagging)\n",
    "pos_input = Input(shape=(maxlen, ), name='pos_input')\n",
    "pos_embedding = Embedding(input_dim=pos_dim, output_dim=pos_dim, init='uniform')(pos_input)\n",
    "combined_input = merge([word_embedding, pos_embedding], mode='concat', concat_axis=2)\n",
    "\n",
    "lstm1 = LSTM(512, return_sequences= , dropout_W=0.2, dropout_U=0.2, input_shape=(maxlen, embedding_dim+pos_dim))(combined_input)\n",
    "lstm2 = LSTM(512, return_sequences=False)(lstm1)\n",
    "drop1 = Dropout(0.5)(lstm2)\n",
    "output = Dense(output_dim, activation='softmax', name='output')(drop1)\n",
    "model = Model(input=[word_input, pos_input], output=output)\n",
    "\n",
    "\n",
    "#combined model\n",
    "#model = Sequential()\n",
    "#model.add(embedding)\n",
    "#model.add(LSTM(512, return_sequences=True, dropout_W=0.2, dropout_U=0.2, input_shape=(maxlen, embedding_dim)))\n",
    "#model.add(LSTM(512, return_sequences=False))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(output_dim))\n",
    "#model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sequential RNN model\n",
    "\n",
    "##\n",
    "#vocab_size = 2+len(vocab_dict.keys())\n",
    "#maxlen = 9 # must be odd\n",
    "#embedding_dim = 200\n",
    "#hidden_dim = 500\n",
    "#output_dim = len(entity2Int_.keys())\n",
    "\n",
    "#model 1\n",
    "#embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, init='uniform')\n",
    "\n",
    "#combined model\n",
    "#model = Sequential()\n",
    "#model.add(embedding)\n",
    "#model.add(LSTM(512, return_sequences=True, dropout_W=0.2, dropout_U=0.2, input_shape=(maxlen, embedding_dim)))\n",
    "#model.add(LSTM(512, return_sequences=False))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(output_dim))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contextwin(l, win):\n",
    "    '''\n",
    "    win :: int corresponding to the size of the window\n",
    "    given a list of indexes composing a sentence\n",
    "\n",
    "    l :: array containing the word indexes\n",
    "\n",
    "    it will return a list of list of indexes corresponding\n",
    "    to context windows surrounding each word in the sentence\n",
    "    '''\n",
    "    assert (win % 2) == 1\n",
    "    assert win >= 1\n",
    "    l = list(l)\n",
    "\n",
    "    lpadded = win // 2 * [-1] + l + win // 2 * [-1]\n",
    "    out = [lpadded[i:(i + win)] for i in range(len(l))]\n",
    "\n",
    "    #print(len(l))\n",
    "    assert len(out) == len(l)\n",
    "    return out\n",
    "\n",
    "#contextwin(TEXT_MAPPING_DATA[1], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9420, 3)\n"
     ]
    }
   ],
   "source": [
    "#prepare training input\n",
    "TRAIN_X = []\n",
    "SIZE_X = []\n",
    "for i in TEXT_MAPPING_DATA:\n",
    "    tmpv = contextwin(i, maxlen)\n",
    "    SIZE_X.append(len(tmpv))    \n",
    "    TRAIN_X.extend(tmpv)\n",
    "\n",
    "print(np.array(TRAIN_X).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 14, 14, 21, 14, 14, 9, 14, 9, 14, 14, 14, 14, 4, 14, 4, 4, 14, 14, 14, 10, 10, 14, 14, 14, 14, 11, 11, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "[14, 14, 14, 21, 14, 14, 9, 14, 9, 14, 14, 14, 14, 4, 14, 4, 4, 14, 14, 14, 10, 10, 14, 14, 14, 14, 11, 11, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "#prepare training output\n",
    "def int2array(int_v, length):\n",
    "    result_v = [0]*length\n",
    "    if(int_v != -1):\n",
    "        result_v[int_v] = 1\n",
    "    return result_v\n",
    "\n",
    "TRAIN_Y = []\n",
    "SIZE_Y = []\n",
    "TRAIN_Y_1 = []\n",
    "print(ENTITY_MAPPING_DATA[0])\n",
    "for i in ENTITY_MAPPING_DATA:\n",
    "    tmpv = map(lambda x:int2array(x, output_dim), i)\n",
    "    SIZE_Y.append(len(tmpv))\n",
    "    TRAIN_Y.extend(tmpv)\n",
    "    TRAIN_Y_1.append(tmpv)\n",
    "\n",
    "print(ENTITY_MAPPING_DATA[0])\n",
    "print(TRAIN_Y[0],TRAIN_Y[1],TRAIN_Y[2],TRAIN_Y[3]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 3, 5, 2, 2, 11, 13, 11, 15, 15, 15, 3, 5, 10, 5, 11, 10, 10, 3, 5, 5, 3, 6, 2, 3, 5, 5, 9, 5, 15, 15, 15, 9, 3, 2, 13, 14, 2, 10]\n",
      "([-1, 6, 2], [6, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "TRAIN_A = []\n",
    "SIZE_A = []\n",
    "TRAIN_A_1 = []\n",
    "\n",
    "for i in POS_MAPPING_DATA:\n",
    "    tmpv = contextwin(i, maxlen)    \n",
    "    SIZE_A.append(len(tmpv))\n",
    "    TRAIN_A.extend(tmpv)\n",
    "    TRAIN_A_1.append(tmpv)\n",
    "    \n",
    "print(POS_MAPPING_DATA[0])\n",
    "print(TRAIN_A[0],TRAIN_A[1])    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('POS', (8813, 3))\n",
      "('TEXT', (9420, 3))\n",
      "('ENTITY', (8813, 23))\n",
      "('POS ORI', (322,))\n",
      "('TEXT ORI', (322,))\n",
      "('ENTITY ORI', (322,))\n"
     ]
    }
   ],
   "source": [
    "print(\"POS\",np.array(TRAIN_A).shape)\n",
    "print(\"TEXT\",np.array(TRAIN_X).shape)\n",
    "print(\"ENTITY\",np.array(TRAIN_Y).shape)\n",
    "\n",
    "print(\"POS ORI\", np.array(POS_MAPPING_DATA).shape)\n",
    "print(\"TEXT ORI\", np.array(TEXT_MAPPING_DATA).shape)\n",
    "print(\"ENTITY ORI\", np.array(ENTITY_MAPPING_DATA).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9420, 3)\n",
      "(322,)\n",
      "(322,)\n"
     ]
    }
   ],
   "source": [
    "#prepare training input\n",
    "TRAIN_X = []\n",
    "SIZE_X = []\n",
    "index_X = 0\n",
    "TRAIN_X_1 = []\n",
    "for i in TEXT_MAPPING_DATA:\n",
    "    tmpv = contextwin(i, maxlen)\n",
    "    TRAIN_X.extend(tmpv)\n",
    "    SIZE_X.append(len(tmpv))\n",
    "    TRAIN_X_1.append(tmpv)\n",
    "\n",
    "print(np.array(TRAIN_X).shape)\n",
    "print(np.array(SIZE_X).shape)\n",
    "print(np.array(TRAIN_X_1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 31, 33, 35, 37, 41, 42, 44, 45, 46, 48, 50, 53, 57, 58, 67, 69, 70, 73, 74, 75, 76, 79, 82, 83, 86, 87, 89, 90, 91, 98, 100, 101, 102, 103, 104, 107, 112, 113, 120, 121, 122, 124, 130, 131, 132, 133, 134, 135, 138, 139, 140, 143, 145, 146, 152, 153, 154, 155, 158, 162, 165, 168, 169, 170, 171, 174, 177, 178, 183, 184, 185, 187, 191, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 212, 219, 220, 222, 229, 230, 231, 232, 233, 235, 236, 238, 241, 242, 244, 246, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 262, 264, 265, 267, 268, 272, 273, 275, 276, 277, 281, 282, 285, 288, 290, 293, 297, 299, 300, 301, 302, 305, 312, 313, 320, 321]\n",
      "[1, 2, 4, 5, 13, 20, 21, 22, 25, 26, 27, 29, 30, 32, 34, 36, 38, 39, 40, 43, 47, 49, 51, 52, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 68, 71, 72, 77, 78, 80, 81, 84, 85, 88, 92, 93, 94, 95, 96, 97, 99, 105, 106, 108, 109, 110, 111, 114, 115, 116, 117, 118, 119, 123, 125, 126, 127, 128, 129, 136, 137, 141, 142, 144, 147, 148, 149, 150, 151, 156, 157, 159, 160, 161, 163, 164, 166, 167, 172, 173, 175, 176, 179, 180, 181, 182, 186, 188, 189, 190, 195, 196, 197, 198, 207, 211, 213, 214, 215, 216, 217, 218, 221, 223, 224, 225, 226, 227, 228, 234, 237, 239, 240, 243, 245, 247, 248, 253, 261, 263, 266, 269, 270, 271, 274, 278, 279, 280, 283, 284, 286, 287, 289, 291, 292, 294, 295, 296, 298, 303, 304, 306, 307, 308, 309, 310, 311, 314, 315, 316, 317, 318, 319]\n",
      "(9420, 3)\n",
      "(8813, 23)\n",
      "(8813, 3)\n",
      "(4175, 3)\n",
      "(4175, 23)\n",
      "(4175, 3)\n"
     ]
    }
   ],
   "source": [
    "i_not_in = []\n",
    "for i in range(len(SIZE_X)):\n",
    "    if SIZE_X[i] != SIZE_Y[i]:\n",
    "        ##print(i, SIZE_X[i], SIZE_Y[i])\n",
    "        i_not_in.append(i)\n",
    "i_in = [i for i in range(len(SIZE_X)) if i not in i_not_in]\n",
    "print(i_not_in)\n",
    "print(i_in)\n",
    "\n",
    "TRAIN_X_2 = []\n",
    "TRAIN_Y_2 = []\n",
    "TRAIN_A_2 = []\n",
    "for i in range(len(SIZE_X)):\n",
    "    if i in i_in:\n",
    "        TRAIN_X_2.extend(TRAIN_X_1[i])\n",
    "        TRAIN_Y_2.extend(TRAIN_Y_1[i])\n",
    "        TRAIN_A_2.extend(TRAIN_A_1[i])\n",
    "\n",
    "print(np.array(TRAIN_X).shape)\n",
    "print(np.array(TRAIN_Y).shape)\n",
    "print(np.array(TRAIN_A).shape)\n",
    "\n",
    "print(np.array(TRAIN_X_2).shape)\n",
    "print(np.array(TRAIN_Y_2).shape)\n",
    "print(np.array(TRAIN_A_2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('POS', array([-1,  6,  2]))\n",
      "('POS', [-1, 6, 2])\n",
      "('TEXT', array([  -1,  490, 2377]))\n",
      "('ENTITY', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(\"POS\",np.array(TRAIN_A)[0])\n",
    "print(\"POS\",(TRAIN_A)[0])\n",
    "print(\"TEXT\",np.array(TRAIN_X_2)[0])\n",
    "print(\"ENTITY\",np.array(TRAIN_Y_2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEXT', array([   0,  490, 2377]))\n",
      "('TEXT', array([0, 6, 2]))\n"
     ]
    }
   ],
   "source": [
    "TRAIN_X_2_NP =  np.array(TRAIN_X_2) \n",
    "TRAIN_X_2_NP[TRAIN_X_2_NP == -1] =0\n",
    "print(\"TEXT\",np.array(TRAIN_X_2_NP)[0])\n",
    "\n",
    "TRAIN_A_2_NP =  np.array(TRAIN_A_2) \n",
    "TRAIN_A_2_NP[TRAIN_A_2_NP == -1] =0\n",
    "print(\"TEXT\",np.array(TRAIN_A_2_NP)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(TRAIN_x), len(TRAIN_Y))\n",
    "#pred_v = model.predict(TRAIN_X)\n",
    "#pred_v.shape\n",
    "#print(model.predict_classes(TRAIN_X))\n",
    "#print(ENTITY_MAPPING_DATA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3757 samples, validate on 418 samples\n",
      "Epoch 1/20\n",
      "3757/3757 [==============================] - 32s - loss: 0.7534 - acc: 0.8411 - val_loss: 0.5116 - val_acc: 0.8445\n",
      "Epoch 2/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.4481 - acc: 0.8805 - val_loss: 0.4178 - val_acc: 0.8947\n",
      "Epoch 3/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.3364 - acc: 0.9079 - val_loss: 0.4404 - val_acc: 0.8995\n",
      "Epoch 4/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.2629 - acc: 0.9271 - val_loss: 0.3725 - val_acc: 0.9067\n",
      "Epoch 5/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.2067 - acc: 0.9388 - val_loss: 0.4357 - val_acc: 0.9091\n",
      "Epoch 6/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.1677 - acc: 0.9532 - val_loss: 0.4861 - val_acc: 0.9091\n",
      "Epoch 7/20\n",
      "3757/3757 [==============================] - 28s - loss: 0.1316 - acc: 0.9614 - val_loss: 0.4980 - val_acc: 0.8971\n",
      "Epoch 8/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.1147 - acc: 0.9662 - val_loss: 0.4997 - val_acc: 0.8900\n",
      "Epoch 9/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0957 - acc: 0.9726 - val_loss: 0.5496 - val_acc: 0.8971\n",
      "Epoch 10/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0855 - acc: 0.9744 - val_loss: 0.6431 - val_acc: 0.8517\n",
      "Epoch 11/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0711 - acc: 0.9790 - val_loss: 0.5505 - val_acc: 0.8852\n",
      "Epoch 12/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0634 - acc: 0.9814 - val_loss: 0.5550 - val_acc: 0.8732\n",
      "Epoch 13/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0572 - acc: 0.9824 - val_loss: 0.7023 - val_acc: 0.8565\n",
      "Epoch 14/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0452 - acc: 0.9862 - val_loss: 0.6366 - val_acc: 0.9019\n",
      "Epoch 15/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0357 - acc: 0.9891 - val_loss: 0.7466 - val_acc: 0.8660\n",
      "Epoch 16/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0424 - acc: 0.9888 - val_loss: 0.6850 - val_acc: 0.8923\n",
      "Epoch 17/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0344 - acc: 0.9904 - val_loss: 0.7408 - val_acc: 0.8708\n",
      "Epoch 18/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0334 - acc: 0.9915 - val_loss: 0.7513 - val_acc: 0.8612\n",
      "Epoch 19/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0256 - acc: 0.9907 - val_loss: 0.7532 - val_acc: 0.8804\n",
      "Epoch 20/20\n",
      "3757/3757 [==============================] - 27s - loss: 0.0261 - acc: 0.9917 - val_loss: 0.7153 - val_acc: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138de0e10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit({'word_input':TRAIN_X_2_NP, 'pos_input':TRAIN_A_2_NP}, {'output':np.array(TRAIN_Y_2)}, nb_epoch=20, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fay/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 9420 input samples and 8813 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-e2381cd3a1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'word_input'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/fay/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fay/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1315\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1319\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fay/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    233\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mset_w\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise ValueError('Sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 9420 input samples and 8813 target samples."
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit({'word_input':np.array(TRAIN_X)}, {'output':np.array(TRAIN_Y)}, nb_epoch=5, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "def saveModel(model):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"./Model/model-keras1.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"./Model/model-keras1.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "saveModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
